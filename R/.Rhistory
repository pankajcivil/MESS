#===============================================================================
# Copyright 2017 Tony Wong
#
# MESS is free software: you can redistribute it and/or modify it under the
# terms of the GNU General Public License as published by the Free Software
# Foundation, either version 3 of the License, or (at your option) any later
# version.
#
# MESS is distributed in the hope that it will be useful, but WITHOUT ANY
# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR
# A PARTICULAR PURPOSE.  See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License along with
# MESS.  If not, see <http://www.gnu.org/licenses/>.
#===============================================================================
filename.saveprogress <- '../output/processing_norfolk.RData'
print('starting to process Norfolk (Sewells Point) tide gauge data')
#===
#=== read in tide gauge data, they're all already hourly series
#===
dat.dir <- '~/codes/EVT/data/tide_gauge_SewellsPoint/'
filetype <- 'txt'
septype <- '\t'
files.tg <- list.files(path=dat.dir,pattern=filetype)
data <- read.table(paste(dat.dir,files.tg[1],sep=''), header = TRUE, sep=septype)
if(length(files.tg) > 1) {
for (ff in 2:length(files.tg)) {
data <- rbind(data, read.table(paste(dat.dir,files.tg[ff],sep=''), header = TRUE, sep=septype))
}
}
# convert sea levels from m to mm, consistent with the other data
data$sl <- 1000* data$sl
data$year   <- as.numeric(substr(as.character(data$date), start=1, stop=4))
data$month  <- as.numeric(substr(as.character(data$date), start=5, stop=6))
data$day    <- as.numeric(substr(as.character(data$date), start=7, stop=8))
# where is the ':' in the times? tells us where to separate off hte substring
# giving the hour of the observations. and cue the jokes about this variable name.
colon.location <- regexpr(':', data$time)
ind.one.digit <- which(colon.location==2)
ind.two.digit <- which(colon.location==3)
data$hour <- rep(NA, length(data$day))
data$hour[ind.one.digit] <- as.numeric(substr(data$time[ind.one.digit], 1,1))
data$hour[ind.two.digit] <- as.numeric(substr(data$time[ind.two.digit], 1,2))
# time in days since 01 January 1960
data$time.days <- as.numeric(mdy.date(month=data$month, day=data$day, year=data$year)) + data$hour/24
# create the object to hold the calibration information for the Norfolk site
# 2 dimensions, one for 'gev_year' experiment information, and one for 'gpd'
# experiment information
data_norfolk <- vector('list', 2)
names(data_norfolk) <- c('gpd','gev_year')
#
#===============================================================================
# daily POT series, for PP-GPD model
#===============================================================================
#
tbeg <- proc.time()
# difference between time stamps (in units of days)
time.diff <- diff(data$time.days)
# check that they are in the proper order, ascending
print(paste('Are there any times out of order? ',any(time.diff < 0), sep=''))
# put everything back in order - make sure you do this for the sea levels and
# other fields too, so do as a matrix. also recalculate the time differences,
# which you will need for averaging
# Note - none for Sewells Point/Norfolk data
#data <- data[order(data$time.days),]
#time.diff <- diff(data$time.days)
# what is one hour? in units of days
one.hours <- 1/24
# where are there gaps longer than one hours? (+10sec for precision)
igap <- which(time.diff > (one.hours+10/(24*60*60)))
#===
#=== subtract linear sea-level trend (from fit to monthly means)
#===
# calculate monthly means
dates.new <- date.mdy(data$time.days)
date.beg <- date.mdy(min(data$time.days))
date.end <- date.mdy(max(data$time.days))
# what the years in which we have data?
years.unique <- unique(dates.new$year)
# in each year, what are the months with at least 90% of the data?
months.this.year <- vector('list', length(years.unique))
names(months.this.year) <- years.unique
years.to.remove <- NULL
for (year in years.unique) {
ind.this.year <- which(dates.new$year == year)
months.to.keep <- NULL
for (month in 1:12) {
ind.this.month <- which(dates.new$month[ind.this.year] == month)
days.this.month <- monthDays(paste(year,'-',month,'-','1', sep=''))
hours.this.month <- days.this.month * 24
perc.data.this.month <- length(ind.this.month)/hours.this.month
if (perc.data.this.month >= 0.9) {months.to.keep <- c(months.to.keep, month)}
}
if(length(months.to.keep)>0) {months.this.year[[year]] <- months.to.keep }
else                         {years.to.remove <- c(years.to.remove, year)}
}
if(length(years.to.remove)>0) {years.unique <- years.unique[-match(years.to.remove, years.unique)]}
# get the mean time (in days releative to 1 Jan 1960) of the observations of
# each month we are using to fit the trend for SLR
times.month <- rep(NA, length(unlist(months.this.year)))
sl.month    <- rep(NA, length(unlist(months.this.year)))
cnt <- 1
for (year in years.unique) {
ind.this.year <- which(dates.new$year == year)
for (month in months.this.year[[year]]) {
ind.this.month <- which(dates.new$month[ind.this.year] == month)
times.month[cnt] <- mean(data$time.days[ind.this.year[ind.this.month]])
sl.month[cnt]    <- mean(data$sl[ind.this.year[ind.this.month]])
cnt <- cnt + 1
}
}
# fit trend
slr.trend <- lm(sl.month ~ times.month)
slr.trend.1hour <- slr.trend$coefficients[1] + slr.trend$coefficients[2]*data$time.days
# subtract off from the 1-hourly data
data$sl.detrended <- data$sl - slr.trend.1hour
print('  ... done.')
#===
#=== daily block maxima; calculate 99% quantile as GPD threshold
#===
# how many days in each year have at least 90% of their values?
days.all <- floor(data$time.days)
days.unique <- unique(days.all)
ind.days.to.remove <- NULL
print('... filtering down to do a daily maxima time series of only the days with at least 90% of data ...')
pb <- txtProgressBar(min=min(days.unique),max=max(days.unique),initial=0,style=3)
for (day in days.unique) {
ind.today <- which(floor(data$time.days) == day)
perc.data.today <- length(ind.today)/24
if(perc.data.today < 0.9) {ind.days.to.remove <- c(ind.days.to.remove, match(day, days.unique))}
setTxtProgressBar(pb, day)
}
close(pb)
days.daily.max <- days.unique[-ind.days.to.remove]
n.days <- length(days.daily.max)
# calculate the daily maximum sea levels on the days of 'days.daily.max'
sl.daily.max <- rep(NA, n.days)
years.daily.max <- rep(NA, n.days)
print('... calculating time series of daily maxima ...')
pb <- txtProgressBar(min=0,max=n.days,initial=0,style=3)
for (day in days.daily.max) {
cnt <- match(day,days.daily.max)
ind.today <- which(days.all == day)
sl.daily.max[cnt] <- max(data$sl.detrended[ind.today])
years.daily.max[cnt] <- data$year[ind.today][1]
setTxtProgressBar(pb, cnt)
}
close(pb)
#===
#=== find all the excesses, "declustering" = if two are within a day of each
#=== other, take only the maximum of the two (so make sure you save the times
#=== of each excess)
#===
print('... getting threshold excesses ...')
# threshold is 99% quantile of tide gauge's observed values.
# Buchanan et al (2016) use the 99% quantile of the daily maximum time series.
#gpd.threshold <- as.numeric(quantile(data$sl.detrended, .99, na.rm=TRUE))
gpd.threshold <- as.numeric(quantile(sl.daily.max, .99, na.rm=TRUE))
data_norfolk$dt.decluster <- 7
ind.exceed <- which(sl.daily.max > gpd.threshold)
days.exceed <- days.daily.max[ind.exceed]
sl.exceed <- sl.daily.max[ind.exceed]
years.exceed <- years.daily.max[ind.exceed]
declustered.exceed <- decluster_timeseries(time=days.exceed, year=years.exceed, time.series=sl.exceed, min.dt=data_norfolk$dt.decluster)
days.exceed.decl <- declustered.exceed$time
years.exceed.decl <- declustered.exceed$year
sl.exceed.decl <- declustered.exceed$time.series
#===
#=== sub-list object on 'data_norfolk' to store what is needed to calbirate PP-GPD
#===
data_norfolk$gpd <- vector('list', 5)
names(data_norfolk$gpd) <- c('counts','year','time_length','excesses','threshold')
# initialize
data_norfolk$gpd$threshold <- gpd.threshold
data_norfolk$gpd$counts <- data_norfolk$gpd$year <- data_norfolk$gpd$time_length <- rep(NA, length(years.unique))
data_norfolk$gpd$excesses <- vector('list', length(years.unique))
for (ind.year in 1:length(years.unique)) {
ind.hits.this.year <- which(years.exceed.decl == years.unique[ind.year])
data_norfolk$gpd$counts[ind.year] <- length(ind.hits.this.year)
data_norfolk$gpd$year[ind.year]   <- years.unique[ind.year]
data_norfolk$gpd$time_length[ind.year] <- length(which(years.daily.max == years.unique[ind.year]))
if(length(ind.hits.this.year) > 0) {data_norfolk$gpd$excesses[[ind.year]] <- sl.exceed.decl[ind.hits.this.year]
} else                             {data_norfolk$gpd$excesses[[ind.year]] <- NA}
}
# alternatively, could bin em all together. but this won't allow for potential
# non-stationary behavior in the poisson process
data_norfolk$gpd$excesses_all <- sl.exceed.decl
data_norfolk$gpd$counts_all <- length(sl.exceed.decl)
data_norfolk$gpd$time_length_all <- length(days.daily.max)
# that takes some time, so save the workspace image after each data set
save.image(file=filename.saveprogress)
#
#===============================================================================
# subsample smaller sets of the POT/GPD data
#===============================================================================
#
# initialize, and create list elements for these GPD experiments. then later
# remove from each sub-list item the years the experiment will not use
gpd.experiments <- c('gpd30','gpd50','gpd70','gpd89')
years.gpd.experiments <- c(30,50,70,89); names(years.gpd.experiments) <- gpd.experiments
for (gpd.exp in gpd.experiments) {
data_norfolk[[gpd.exp]] <- data_norfolk$gpd
ind.experiment <- (length(data_norfolk$gpd$year)-years.gpd.experiments[[gpd.exp]]+1):length(data_norfolk$gpd$year)
data_norfolk[[gpd.exp]]$counts <- data_norfolk[[gpd.exp]]$counts[ind.experiment]
data_norfolk[[gpd.exp]]$time_length <- data_norfolk[[gpd.exp]]$time_length[ind.experiment]
data_norfolk[[gpd.exp]]$excesses <- data_norfolk[[gpd.exp]]$excesses[ind.experiment]
data_norfolk[[gpd.exp]]$year <- data_norfolk$gpd$year[ind.experiment]
data_norfolk[[gpd.exp]]$counts_all <- NULL
data_norfolk[[gpd.exp]]$time_length_all <- NULL
data_norfolk[[gpd.exp]]$excesses_all <- NULL
}
tend <- proc.time()
print(paste('  ... done. Took ', (tend[3]-tbeg[3])/60, ' minutes.',sep=''))
#
#===============================================================================
# now do the GEV/Naveau annual block maxima. calculate based on the hourly time
# series (data$sl.detrended)
#===============================================================================
#
# give an update to the screen
print('starting to process annual block maxima for Sewells Point/Norfolk data set')
# set up object for passing through calibration routines
data_norfolk$gev_year <- vector('list', 2)
names(data_norfolk$gev_year) <- c('year','lsl_max')
data_norfolk$gev_year$year <- unique(data$year)
data_norfolk$gev_year$year <- data_norfolk$gev_year$year[order(data_norfolk$gev_year$year)]
data_norfolk$gev_year$lsl_max <- rep(NA, length(data_norfolk$gev_year$year))
for (t in 1:length(data_norfolk$gev_year$year)) {
ind_this_year <- which(data$year==data_norfolk$gev_year$year[t])
data_norfolk$gev_year$lsl_max[t] <- max(data$sl.detrended[ind_this_year])
}
# trim before 1850 (or whenever is time_forc[1]), which is when the temperature
# forcing starts. also make a note of how long each record is
nyear.norfolk <- NA
if(data_norfolk$gev_year$year[1] < time_forc[1]) {
icut <- which(data_norfolk$gev_year$year < time_forc[1])
data_norfolk$gev_year$year <- data_norfolk$gev_year$year[-icut]
data_norfolk$gev_year$lsl_max <- data_norfolk$gev_year$lsl_max[-icut]
}
nyear.norfolk <- length(data_norfolk$gev_year$year)
# that doesn't take as long... so just save it once for good measure
save.image(file=filename.saveprogress)
# save final 'data_norfolk' object to RDS to use later
today=Sys.Date(); today=format(today,format="%d%b%Y")
filename.output <- paste('../data/tidegauge_processed_norfolk_decl',data_norfolk$dt.decluster,'_',today,'.rds', sep='')
saveRDS(data_norfolk, file=filename.output)
#===============================================================================
print('done processing the Sewells Point/Norfolk tide gauge data set')
#===============================================================================
# End
#===============================================================================
#===============================================================================
# processing of Norfolk, Virginia, USA, data.
#
# leads to the list object 'data_norfolk', analogous to the 'data_many' object
# that contains the many tide gauge stations from UHSLC database;
# has all of the necessary information to estimate (MLE) the PP/GPD parameters
# for each of the 4 candidate model structures.
#
# Questions? Tony Wong (twong@psu.edu)
#===============================================================================
#===============================================================================
# Copyright 2017 Tony Wong
#
# MESS is free software: you can redistribute it and/or modify it under the
# terms of the GNU General Public License as published by the Free Software
# Foundation, either version 3 of the License, or (at your option) any later
# version.
#
# MESS is distributed in the hope that it will be useful, but WITHOUT ANY
# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR
# A PARTICULAR PURPOSE.  See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License along with
# MESS.  If not, see <http://www.gnu.org/licenses/>.
#===============================================================================
filename.saveprogress <- '../output/processing_norfolk.RData'
print('starting to process Norfolk (Sewells Point) tide gauge data')
#===
#=== read in tide gauge data, they're all already hourly series
#===
dat.dir <- '~/codes/EVT/data/tide_gauge_SewellsPoint/'
filetype <- 'txt'
septype <- '\t'
files.tg <- list.files(path=dat.dir,pattern=filetype)
data <- read.table(paste(dat.dir,files.tg[1],sep=''), header = TRUE, sep=septype)
if(length(files.tg) > 1) {
for (ff in 2:length(files.tg)) {
data <- rbind(data, read.table(paste(dat.dir,files.tg[ff],sep=''), header = TRUE, sep=septype))
}
}
# convert sea levels from m to mm, consistent with the other data
data$sl <- 1000* data$sl
data$year   <- as.numeric(substr(as.character(data$date), start=1, stop=4))
data$month  <- as.numeric(substr(as.character(data$date), start=5, stop=6))
data$day    <- as.numeric(substr(as.character(data$date), start=7, stop=8))
# where is the ':' in the times? tells us where to separate off hte substring
# giving the hour of the observations. and cue the jokes about this variable name.
colon.location <- regexpr(':', data$time)
ind.one.digit <- which(colon.location==2)
ind.two.digit <- which(colon.location==3)
data$hour <- rep(NA, length(data$day))
data$hour[ind.one.digit] <- as.numeric(substr(data$time[ind.one.digit], 1,1))
data$hour[ind.two.digit] <- as.numeric(substr(data$time[ind.two.digit], 1,2))
# time in days since 01 January 1960
data$time.days <- as.numeric(mdy.date(month=data$month, day=data$day, year=data$year)) + data$hour/24
# create the object to hold the calibration information for the Norfolk site
# 2 dimensions, one for 'gev_year' experiment information, and one for 'gpd'
# experiment information
data_norfolk <- vector('list', 2)
names(data_norfolk) <- c('gpd','gev_year')
#
#===============================================================================
# daily POT series, for PP-GPD model
#===============================================================================
#
tbeg <- proc.time()
# difference between time stamps (in units of days)
time.diff <- diff(data$time.days)
# check that they are in the proper order, ascending
print(paste('Are there any times out of order? ',any(time.diff < 0), sep=''))
# put everything back in order - make sure you do this for the sea levels and
# other fields too, so do as a matrix. also recalculate the time differences,
# which you will need for averaging
# Note - none for Sewells Point/Norfolk data
#data <- data[order(data$time.days),]
#time.diff <- diff(data$time.days)
# what is one hour? in units of days
one.hours <- 1/24
# where are there gaps longer than one hours? (+10sec for precision)
igap <- which(time.diff > (one.hours+10/(24*60*60)))
#===
#=== subtract linear sea-level trend (from fit to monthly means)
#===
# calculate monthly means
dates.new <- date.mdy(data$time.days)
date.beg <- date.mdy(min(data$time.days))
date.end <- date.mdy(max(data$time.days))
# what the years in which we have data?
years.unique <- unique(dates.new$year)
# in each year, what are the months with at least 90% of the data?
months.this.year <- vector('list', length(years.unique))
names(months.this.year) <- years.unique
years.to.remove <- NULL
for (year in years.unique) {
ind.this.year <- which(dates.new$year == year)
months.to.keep <- NULL
for (month in 1:12) {
ind.this.month <- which(dates.new$month[ind.this.year] == month)
days.this.month <- monthDays(paste(year,'-',month,'-','1', sep=''))
hours.this.month <- days.this.month * 24
perc.data.this.month <- length(ind.this.month)/hours.this.month
if (perc.data.this.month >= 0.9) {months.to.keep <- c(months.to.keep, month)}
}
if(length(months.to.keep)>0) {months.this.year[[year]] <- months.to.keep }
else                         {years.to.remove <- c(years.to.remove, year)}
}
if(length(years.to.remove)>0) {years.unique <- years.unique[-match(years.to.remove, years.unique)]}
# get the mean time (in days releative to 1 Jan 1960) of the observations of
# each month we are using to fit the trend for SLR
times.month <- rep(NA, length(unlist(months.this.year)))
sl.month    <- rep(NA, length(unlist(months.this.year)))
cnt <- 1
for (year in years.unique) {
ind.this.year <- which(dates.new$year == year)
for (month in months.this.year[[year]]) {
ind.this.month <- which(dates.new$month[ind.this.year] == month)
times.month[cnt] <- mean(data$time.days[ind.this.year[ind.this.month]])
sl.month[cnt]    <- mean(data$sl[ind.this.year[ind.this.month]])
cnt <- cnt + 1
}
}
# fit trend
slr.trend <- lm(sl.month ~ times.month)
slr.trend.1hour <- slr.trend$coefficients[1] + slr.trend$coefficients[2]*data$time.days
# subtract off from the 1-hourly data
data$sl.detrended <- data$sl - slr.trend.1hour
print('  ... done.')
#===
#=== daily block maxima; calculate 99% quantile as GPD threshold
#===
# how many days in each year have at least 90% of their values?
days.all <- floor(data$time.days)
days.unique <- unique(days.all)
ind.days.to.remove <- NULL
print('... filtering down to do a daily maxima time series of only the days with at least 90% of data ...')
pb <- txtProgressBar(min=min(days.unique),max=max(days.unique),initial=0,style=3)
for (day in days.unique) {
ind.today <- which(floor(data$time.days) == day)
perc.data.today <- length(ind.today)/24
if(perc.data.today < 0.9) {ind.days.to.remove <- c(ind.days.to.remove, match(day, days.unique))}
setTxtProgressBar(pb, day)
}
close(pb)
days.daily.max <- days.unique[-ind.days.to.remove]
n.days <- length(days.daily.max)
# calculate the daily maximum sea levels on the days of 'days.daily.max'
sl.daily.max <- rep(NA, n.days)
years.daily.max <- rep(NA, n.days)
print('... calculating time series of daily maxima ...')
pb <- txtProgressBar(min=0,max=n.days,initial=0,style=3)
for (day in days.daily.max) {
cnt <- match(day,days.daily.max)
ind.today <- which(days.all == day)
sl.daily.max[cnt] <- max(data$sl.detrended[ind.today])
years.daily.max[cnt] <- data$year[ind.today][1]
setTxtProgressBar(pb, cnt)
}
close(pb)
#===
#=== find all the excesses, "declustering" = if two are within a day of each
#=== other, take only the maximum of the two (so make sure you save the times
#=== of each excess)
#===
print('... getting threshold excesses ...')
# threshold is 99% quantile of tide gauge's observed values.
# Buchanan et al (2016) use the 99% quantile of the daily maximum time series.
#gpd.threshold <- as.numeric(quantile(data$sl.detrended, .99, na.rm=TRUE))
gpd.threshold <- as.numeric(quantile(sl.daily.max, .99, na.rm=TRUE))
data_norfolk$dt.decluster <- 9
ind.exceed <- which(sl.daily.max > gpd.threshold)
days.exceed <- days.daily.max[ind.exceed]
sl.exceed <- sl.daily.max[ind.exceed]
years.exceed <- years.daily.max[ind.exceed]
declustered.exceed <- decluster_timeseries(time=days.exceed, year=years.exceed, time.series=sl.exceed, min.dt=data_norfolk$dt.decluster)
days.exceed.decl <- declustered.exceed$time
years.exceed.decl <- declustered.exceed$year
sl.exceed.decl <- declustered.exceed$time.series
#===
#=== sub-list object on 'data_norfolk' to store what is needed to calbirate PP-GPD
#===
data_norfolk$gpd <- vector('list', 5)
names(data_norfolk$gpd) <- c('counts','year','time_length','excesses','threshold')
# initialize
data_norfolk$gpd$threshold <- gpd.threshold
data_norfolk$gpd$counts <- data_norfolk$gpd$year <- data_norfolk$gpd$time_length <- rep(NA, length(years.unique))
data_norfolk$gpd$excesses <- vector('list', length(years.unique))
for (ind.year in 1:length(years.unique)) {
ind.hits.this.year <- which(years.exceed.decl == years.unique[ind.year])
data_norfolk$gpd$counts[ind.year] <- length(ind.hits.this.year)
data_norfolk$gpd$year[ind.year]   <- years.unique[ind.year]
data_norfolk$gpd$time_length[ind.year] <- length(which(years.daily.max == years.unique[ind.year]))
if(length(ind.hits.this.year) > 0) {data_norfolk$gpd$excesses[[ind.year]] <- sl.exceed.decl[ind.hits.this.year]
} else                             {data_norfolk$gpd$excesses[[ind.year]] <- NA}
}
# alternatively, could bin em all together. but this won't allow for potential
# non-stationary behavior in the poisson process
data_norfolk$gpd$excesses_all <- sl.exceed.decl
data_norfolk$gpd$counts_all <- length(sl.exceed.decl)
data_norfolk$gpd$time_length_all <- length(days.daily.max)
# that takes some time, so save the workspace image after each data set
save.image(file=filename.saveprogress)
#
#===============================================================================
# subsample smaller sets of the POT/GPD data
#===============================================================================
#
# initialize, and create list elements for these GPD experiments. then later
# remove from each sub-list item the years the experiment will not use
gpd.experiments <- c('gpd30','gpd50','gpd70','gpd89')
years.gpd.experiments <- c(30,50,70,89); names(years.gpd.experiments) <- gpd.experiments
for (gpd.exp in gpd.experiments) {
data_norfolk[[gpd.exp]] <- data_norfolk$gpd
ind.experiment <- (length(data_norfolk$gpd$year)-years.gpd.experiments[[gpd.exp]]+1):length(data_norfolk$gpd$year)
data_norfolk[[gpd.exp]]$counts <- data_norfolk[[gpd.exp]]$counts[ind.experiment]
data_norfolk[[gpd.exp]]$time_length <- data_norfolk[[gpd.exp]]$time_length[ind.experiment]
data_norfolk[[gpd.exp]]$excesses <- data_norfolk[[gpd.exp]]$excesses[ind.experiment]
data_norfolk[[gpd.exp]]$year <- data_norfolk$gpd$year[ind.experiment]
data_norfolk[[gpd.exp]]$counts_all <- NULL
data_norfolk[[gpd.exp]]$time_length_all <- NULL
data_norfolk[[gpd.exp]]$excesses_all <- NULL
}
tend <- proc.time()
print(paste('  ... done. Took ', (tend[3]-tbeg[3])/60, ' minutes.',sep=''))
#
#===============================================================================
# now do the GEV/Naveau annual block maxima. calculate based on the hourly time
# series (data$sl.detrended)
#===============================================================================
#
# give an update to the screen
print('starting to process annual block maxima for Sewells Point/Norfolk data set')
# set up object for passing through calibration routines
data_norfolk$gev_year <- vector('list', 2)
names(data_norfolk$gev_year) <- c('year','lsl_max')
data_norfolk$gev_year$year <- unique(data$year)
data_norfolk$gev_year$year <- data_norfolk$gev_year$year[order(data_norfolk$gev_year$year)]
data_norfolk$gev_year$lsl_max <- rep(NA, length(data_norfolk$gev_year$year))
for (t in 1:length(data_norfolk$gev_year$year)) {
ind_this_year <- which(data$year==data_norfolk$gev_year$year[t])
data_norfolk$gev_year$lsl_max[t] <- max(data$sl.detrended[ind_this_year])
}
# trim before 1850 (or whenever is time_forc[1]), which is when the temperature
# forcing starts. also make a note of how long each record is
nyear.norfolk <- NA
if(data_norfolk$gev_year$year[1] < time_forc[1]) {
icut <- which(data_norfolk$gev_year$year < time_forc[1])
data_norfolk$gev_year$year <- data_norfolk$gev_year$year[-icut]
data_norfolk$gev_year$lsl_max <- data_norfolk$gev_year$lsl_max[-icut]
}
nyear.norfolk <- length(data_norfolk$gev_year$year)
# that doesn't take as long... so just save it once for good measure
save.image(file=filename.saveprogress)
# save final 'data_norfolk' object to RDS to use later
today=Sys.Date(); today=format(today,format="%d%b%Y")
filename.output <- paste('../data/tidegauge_processed_norfolk_decl',data_norfolk$dt.decluster,'_',today,'.rds', sep='')
saveRDS(data_norfolk, file=filename.output)
#===============================================================================
print('done processing the Sewells Point/Norfolk tide gauge data set')
#===============================================================================
# End
#===============================================================================
